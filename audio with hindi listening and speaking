import openai
import streamlit as st
import os
import time
import torch
import torchaudio
from transformers import VitsModel, AutoTokenizer, WhisperForConditionalGeneration, WhisperProcessor, pipeline
from peft import PeftModel, PeftConfig

# Load OpenAI API Key
openai.api_key = 'api_key_here'
# Load Whisper ASR Model
peft_model_id = "kasunw/whisper-large-v3-hindi"
peft_config = PeftConfig.from_pretrained(peft_model_id)

asr_model = WhisperForConditionalGeneration.from_pretrained(
    peft_config.base_model_name_or_path, device_map="auto", torch_dtype=torch.float16
)
asr_model = PeftModel.from_pretrained(asr_model, peft_model_id)
asr_model.config.use_cache = True

processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path, language="Hindi", task="transcribe")

torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
asr_pipeline = pipeline(
    "automatic-speech-recognition",
    model=asr_model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    chunk_length_s=30,
    batch_size=16,
    return_timestamps=True,
    torch_dtype=torch_dtype  # No explicit 'device' argument
)

# Load Facebook MMS-TTS Model
tts_model = VitsModel.from_pretrained("facebook/mms-tts-hin")
tts_tokenizer = AutoTokenizer.from_pretrained("facebook/mms-tts-hin")

def speak(text):
    """Convert text to speech using Facebook's MMS-TTS model and play the generated audio."""
    inputs = tts_tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        output = tts_model(**inputs).waveform  # Generate speech waveform

    # Save and play the output audio
    output_file = "response.wav"
    torchaudio.save(output_file, output, sample_rate=16000)  
    os.system(f"afplay {output_file}")  # macOS (Use 'aplay' for Linux)

import sounddevice as sd
import wave
import numpy as np

def record_audio(filename="audio.wav", duration=5, samplerate=16000):
    """Records audio from the microphone and saves it as a WAV file."""
    st.info(f"Recording {duration} seconds of audio...")

    audio_data = sd.rec(int(samplerate * duration), samplerate=samplerate, channels=1, dtype=np.int16)
    sd.wait()  # Wait until recording is finished

    # Save as WAV file
    with wave.open(filename, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)  # 16-bit audio
        wf.setframerate(samplerate)
        wf.writeframes(audio_data.tobytes())

    st.success("Recording saved!")

def listen():
    """Record, transcribe, and return text."""
    record_audio()  # Record audio and save as "audio.wav"

    # Check if the file exists before processing
    if not os.path.exists("audio.wav"):
        st.error("Error: Audio file not found!")
        return None

    result = asr_pipeline("audio.wav")  # Transcribe using Whisper

    return result["text"] if "text" in result else None

def get_gpt_response(query):
    """Get a response from OpenAI's GPT model."""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": query},
            ],
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        return f"Error: {str(e)}"

# Streamlit UI
st.title("AI Voice Chat (Whisper ASR + MMS TTS)")
st.write("Ask a question using text or voice!")

user_input = st.text_input("Enter your question:")
if st.button("Submit"):
    if user_input:
        response = get_gpt_response(user_input)
        st.success(response)
        speak(response)

# Use session state to keep track of listening mode
if "listening" not in st.session_state:
    st.session_state.listening = False

# Toggle button for continuous listening
if st.button("Start/Stop Listening"):
    st.session_state.listening = not st.session_state.listening

if st.session_state.listening:
    st.warning("Listening mode is ON. Speak now!")
    while st.session_state.listening:
        query = listen()
        if query:
            st.text(f"You said: {query}")
            response = get_gpt_response(query)
            st.success(response)
            speak(response)
        time.sleep(1)  # Prevent excessive looping
